{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbnbNightlyPriceRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        assert len(X) == len(y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X.iloc[index].values, dtype=torch.float32), torch.tensor(self.y.iloc[index], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression_jared(torch.nn.Module):#\n",
    "    def __init__(self, config_model_structure):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        config_model_structure\n",
    "    )\n",
    "\n",
    "    def forward(self, X):\n",
    "        #return prediction \n",
    "        return self.layers(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_structure(config):\n",
    "\n",
    "    hidden_layer_size = config['hidden_layer_width']\n",
    "    linear_depth = config['depth']\n",
    "\n",
    "    config_dict = OrderedDict()\n",
    "\n",
    "    #input layer\n",
    "    config_dict['input'] = nn.Linear(11, hidden_layer_size)\n",
    "\n",
    "    for idx in range(linear_depth):\n",
    "        rel_idx = f'relu{idx}'\n",
    "        config_dict[rel_idx] = nn.ReLU()\n",
    "\n",
    "        layer_idx = f'layer{idx}'\n",
    "        config_dict[layer_idx] = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "\n",
    "    #outside the loop from here to add the\n",
    "    config_dict[f'layer{linear_depth}'] = nn.Linear(hidden_layer_size, 11) \n",
    "    config_dict[f'relu{linear_depth}'] = nn.ReLU()\n",
    "\n",
    "    #output layer\n",
    "    config_dict['output'] = nn.Linear(11, 1)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nn_config():\n",
    "    \n",
    "    combined_dictionary = {\n",
    "    'optimiser': ['SGD', 'Adam', 'SparseAdam'],\n",
    "    'lr': [0.01, 0.001, 0.0001],\n",
    "    'hidden_layer_width': [16, 11, 10],\n",
    "    'depth': [5, 3,1]\n",
    "    }\n",
    "\n",
    "    config_dict_list = []\n",
    "    \n",
    "    for iteration in itertools.product(*combined_dictionary.values()):\n",
    "        config_dict = {\n",
    "            'optimiser': iteration[0],\n",
    "            'lr': iteration[1],\n",
    "            'hidden_layer_width': iteration[2],\n",
    "            'depth': iteration[3]}\n",
    "        config_dict_list.append(config_dict)\n",
    "\n",
    "    return config_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, nn_config, epochs=16):\n",
    "\n",
    "    lr = nn_config['lr']\n",
    "    \n",
    "    if nn_config['optimiser'] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "    elif nn_config['optimiser'] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    elif nn_config['optimiser'] == \"SparseAdam\":\n",
    "        optimizer = torch.optim.SparseAdam(model.parameters(), lr)\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "    _batch_idx = 0\n",
    "    \n",
    "    starting_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            features, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(('loss'), loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "            #print(f\"Training Loss: {loss.item()}\")\n",
    "\n",
    "        prediction_time_list = []\n",
    "        for _batch in val_dataloader:\n",
    "            features, labels =  _batch\n",
    "            optimizer.zero_grad()  # Reset gradients before inference\n",
    "            timer_start_ = time.time() # Start timer for interference_latency\n",
    "            predictions = model(features)\n",
    "            timer_end_ = time.time() # End timer for interference_latency\n",
    "            batch_prediction_time = (timer_end_-timer_start_)/len(features) # Calculate interference_latency for each batch\n",
    "            prediction_time_list.append(batch_prediction_time)\n",
    "            loss = criterion(predictions, labels)\n",
    "            writer.add_scalar(('loss_val'), loss.item(), _batch_idx)\n",
    "            _batch_idx += 1\n",
    "\n",
    "    #End of the training_duratioin time \n",
    "    ending_time = datetime.now()\n",
    "            \n",
    "    # getting the timestamp\n",
    "    training_duration =  ending_time - starting_time\n",
    "    time_filename = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    interference_latency =  sum(prediction_time_list) / len(prediction_time_list)\n",
    "\n",
    "    return model, training_duration, interference_latency, time_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(best_model, train_loader, validation_loader, test_loader):\n",
    "    def calculate_metrics_for_loader(loader):\n",
    "        y_hat = []  # Predictions\n",
    "        y = []  # Targets\n",
    "\n",
    "        for features, labels in loader:\n",
    "            features = features.to(torch.float32)  \n",
    "            labels = labels.to(torch.float32)  \n",
    "            prediction = best_model(features)\n",
    "            y.extend(labels.detach().numpy())\n",
    "            y_hat.extend(prediction.detach().numpy())\n",
    "\n",
    "            y = np.array(y)\n",
    "            y_hat = np.array(y_hat)\n",
    "    \n",
    "            # If the predictions include nan values, assign poor metrics to discard the model later\n",
    "            #if np.isnan(y_hat).any():\n",
    "            #    RMSE_loss = 1000000\n",
    "            #    R_squared = 0\n",
    "            #else:  # Else, calculate RMSE and R^2\n",
    "            RMSE_loss = np.sqrt(mean_squared_error(y, y_hat))\n",
    "            R_squared = r2_score(y, y_hat)\n",
    "\n",
    "        return RMSE_loss, R_squared\n",
    "\n",
    "    train_RMSE_loss, train_R_squared = calculate_metrics_for_loader(train_loader)\n",
    "    validation_RMSE_loss, validation_R_squared = calculate_metrics_for_loader(validation_loader)\n",
    "    test_RMSE_loss, test_R_squared = calculate_metrics_for_loader(test_loader)\n",
    "\n",
    "    return train_RMSE_loss, validation_RMSE_loss, test_RMSE_loss, train_R_squared, validation_R_squared, test_R_squared\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, best_hyperparameters_, best_metrics, time_stamp):\n",
    "    dest = '/Users/momo/aicore/github/modelling-airbnbs-property-listing-dataset-/neural_networks/regression'\n",
    "    new_path = f\"{dest}/{time_stamp}\"\n",
    "    os.mkdir(new_path)\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(model.state_dict(), f'{new_path}/model.pt')\n",
    "    \n",
    "    #Save Hyperparameter\n",
    "    hyperparameter = {\n",
    "        'optimiser': best_hyperparameters_['optimiser'],\n",
    "        'lr': best_hyperparameters_['lr'],\n",
    "        'hidden_layer_width': best_hyperparameters_['hidden_layer_width'],\n",
    "        'depth': best_hyperparameters_['depth']\n",
    "    }\n",
    "    \n",
    "    filename = os.path.join(new_path, \"hyperparameters.json\")\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(hyperparameter, json_file)\n",
    "    \n",
    "    # Save the metrics to a JSON file\n",
    "    training_duration = best_metrics['training_duration']\n",
    "        \n",
    "    metrics_data = {\n",
    "    'RMSE_loss_train': best_metrics['RMSE_loss'][0],\n",
    "    'RMSE_loss_validation': best_metrics['RMSE_loss'][1],\n",
    "    'RMSE_loss_test': best_metrics['RMSE_loss'][2],\n",
    "    'R_squared_train': best_metrics['R_squared'][0],\n",
    "    'R_squared_validation': best_metrics['R_squared'][1],\n",
    "    'R_squared_test': best_metrics['R_squared'][2],\n",
    "    'training_duration_seconds': training_duration.total_seconds(),\n",
    "    'inference_latency' : best_metrics['inference_latency']\n",
    "    }\n",
    "\n",
    "    #Save the metrics to a JSON file\n",
    "    metrics_filename = os.path.join(new_path, \"metrics.json\")\n",
    "    with open (metrics_filename, 'w') as json_file:\n",
    "        json.dump(metrics_data, json_file)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_nn(config_dict_list, train_dataloader, validation_dataloader, test_dataloader, model):\n",
    "    \n",
    "    # For each configuration, redefine the nn_model and the training function\n",
    "    for i, nn_config in enumerate(config_dict_list):\n",
    "\n",
    "        best_metrics_ = None\n",
    "        best_hyperparameters = nn_config\n",
    "\n",
    "\n",
    "        # Train the NN model using the model, the dataloaders and nn_config file\n",
    "        best_model, training_duration, inference_latency, time_stamp = train(model, train_dataloader, validation_dataloader, nn_config)\n",
    "\n",
    "        # Calculate the metrics\n",
    "        train_RMSE_loss, validation_RMSE_loss, test_RMSE_loss, train_R_squared, validation_R_squared, test_R_squared = calculate_metrics(best_model, train_dataloader, validation_dataloader, test_dataloader)\n",
    "\n",
    "        best_metrics = {\n",
    "\n",
    "        'RMSE_loss' : [train_RMSE_loss,validation_RMSE_loss,test_RMSE_loss],\n",
    "        'R_squared' : [train_R_squared, validation_R_squared, test_R_squared],\n",
    "        'training_duration' : training_duration,\n",
    "        'inference_latency' : inference_latency,\n",
    "    }\n",
    "        # Store the metrics, config, and model:\n",
    "        if best_metrics_ == None or best_metrics.get('R_squared')[1]>best_metrics_.get('R_squared')[1]:\n",
    "            best_model_ = best_model\n",
    "            best_hyperparameters_ = best_hyperparameters\n",
    "            best_metrics_ = best_metrics\n",
    "\n",
    "        if i >= 50:\n",
    "            break\n",
    "        \n",
    "        \n",
    "\n",
    "    save_model(best_model_, best_hyperparameters_, best_metrics_, time_stamp)\n",
    "\n",
    "    print(best_metrics_, best_hyperparameters_)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataframe = pd.read_csv('clean_tabular_data.csv')\n",
    "    X, y = dataframe.drop('Price_Night', axis=1), dataframe['Price_Night']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset\n",
    "X,y = load_data()\n",
    "dataset = AirbnbNightlyPriceRegressionDataset(X, y)\n",
    "\n",
    "\n",
    "# Split tarining, validation, testing samples (80,10,10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "#Dataset\n",
    "train_dataset = AirbnbNightlyPriceRegressionDataset(X_train, y_train)\n",
    "val_dataset = AirbnbNightlyPriceRegressionDataset(X_validation, y_validation)\n",
    "test_dataset = AirbnbNightlyPriceRegressionDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "#DataLoader\n",
    "batch_size = 16 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/momo/miniconda3/envs/project5/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/momo/miniconda3/envs/project5/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/momo/miniconda3/envs/project5/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression_jared(model_structure)\n\u001b[1;32m     12\u001b[0m model, training_duration, interference_latency, time_filename \u001b[38;5;241m=\u001b[39m train(model, train_dataloader, val_dataloader, config)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mfind_best_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#print(created)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[124], line 14\u001b[0m, in \u001b[0;36mfind_best_nn\u001b[0;34m(config_dict_list, train_dataloader, validation_dataloader, test_dataloader, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     best_model, training_duration, inference_latency, time_stamp \u001b[38;5;241m=\u001b[39m train(model, train_dataloader, validation_dataloader, nn_config)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Calculate the metrics\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     train_RMSE_loss, validation_RMSE_loss, test_RMSE_loss, train_R_squared, validation_R_squared, test_R_squared \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     best_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE_loss\u001b[39m\u001b[38;5;124m'\u001b[39m : [train_RMSE_loss,validation_RMSE_loss,test_RMSE_loss],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_latency\u001b[39m\u001b[38;5;124m'\u001b[39m : inference_latency,\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Store the metrics, config, and model:\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[122], line 26\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(best_model, train_loader, validation_loader, test_loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m     R_squared \u001b[38;5;241m=\u001b[39m r2_score(y, y_hat)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RMSE_loss, R_squared\n\u001b[0;32m---> 26\u001b[0m train_RMSE_loss, train_R_squared \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics_for_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m validation_RMSE_loss, validation_R_squared \u001b[38;5;241m=\u001b[39m calculate_metrics_for_loader(validation_loader)\n\u001b[1;32m     28\u001b[0m test_RMSE_loss, test_R_squared \u001b[38;5;241m=\u001b[39m calculate_metrics_for_loader(test_loader)\n",
      "Cell \u001b[0;32mIn[122], line 21\u001b[0m, in \u001b[0;36mcalculate_metrics.<locals>.calculate_metrics_for_loader\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_hat)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# If the predictions include nan values, assign poor metrics to discard the model later\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#if np.isnan(y_hat).any():\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#    RMSE_loss = 1000000\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#    R_squared = 0\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#else:  # Else, calculate RMSE and R^2\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m RMSE_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m R_squared \u001b[38;5;241m=\u001b[39m r2_score(y, y_hat)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RMSE_loss, R_squared\n",
      "File \u001b[0;32m~/miniconda3/envs/project5/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/project5/lib/python3.11/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/project5/lib/python3.11/site-packages/sklearn/metrics/_regression.py:101\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    104\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/project5/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/project5/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/project5/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate configurations\n",
    "    config_dict_list = generate_nn_config()\n",
    "    \n",
    "    # Loop over each configuration\n",
    "    for config in config_dict_list:\n",
    "        # Get model structure based on the current configuration\n",
    "        model_structure = get_model_structure(config)\n",
    "        \n",
    "        # Create LinearRegression_jared model using the model structure\n",
    "        model = LinearRegression_jared(model_structure)\n",
    "        model, training_duration, interference_latency, time_filename = train(model, train_dataloader, val_dataloader, config)\n",
    "        find_best_nn(config_dict_list,train_dataloader, val_dataloader,test_dataloader, model)\n",
    "    \n",
    "        \n",
    "        # Print the model\n",
    "        #print(created)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
